# -*- coding: utf-8 -*-
"""_RandomForest_scratch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18LbVsqnEdz1IG6qdS504ykr1_f9Dsu8Z
"""

import numpy as np
import statistics

# DecisionStump class represents a single, very simple decision tree (a stump).
# It makes a split based on a single feature and a single threshold.
class DecisionStump:
    def fit(self, X, y):
        # Get the number of features in the input data
        n_features = X.shape[1]

        # Randomly select one feature to split on
        self.feature_index = np.random.randint(0, n_features)

        # Calculate the threshold for the chosen feature (e.g., the mean value)
        self.threshold = np.mean(X[:, self.feature_index])

        # Split the target variable 'y' into two groups based on the threshold
        # 'left' contains labels where the feature value is less than or equal to the threshold
        left = y[X[:, self.feature_index] <= self.threshold]
        # 'right' contains labels where the feature value is greater than the threshold
        right = y[X[:, self.feature_index] > self.threshold]

        # Determine the most frequent class (mode) for the 'left' split
        # If the split is empty, default to 0
        self.left_class = statistics.mode(left) if len(left) > 0 else 0
        # Determine the most frequent class (mode) for the 'right' split
        # If the split is empty, default to 0
        self.right_class = statistics.mode(right) if len(right) > 0 else 0

    def predict(self, x):
        # Predict the class for a single data point 'x'
        # Based on whether the feature value falls into the 'left' or 'right' split
        if x[self.feature_index] <= self.threshold:
            return self.left_class
        return self.right_class


# RandomForestClassifier class implements a Random Forest classifier from scratch.
# It builds an ensemble of DecisionStumps and combines their predictions.
class RandomForestClassifier:
    def __init__(self, n_trees=10):
        # Initialize the number of decision stumps (trees) in the forest
        self.n_trees = n_trees
        # List to store the trained decision stumps
        self.trees = []

    def bootstrap(self, X, y):
        # Perform bootstrapping: sample with replacement from the dataset
        # This creates a new dataset of the same size as the original
        idx = np.random.choice(len(X), len(X), replace=True)
        return X[idx], y[idx]

    def fit(self, X, y):
        # Train the random forest by creating and training multiple decision stumps
        self.trees = [] # Clear any existing trees
        for _ in range(self.n_trees):
            # Create a bootstrap sample for each tree
            Xs, ys = self.bootstrap(X, y)
            # Create a new DecisionStump
            stump = DecisionStump()
            # Train the stump on the bootstrap sample
            stump.fit(Xs, ys)
            # Add the trained stump to the forest
            self.trees.append(stump)

    def predict(self, X):
        # Make predictions for a set of data points 'X'
        predictions = []
        for x in X:
            # Get predictions from all individual decision stumps for the current data point
            votes = [tree.predict(x) for tree in self.trees]
            # Combine the predictions using majority voting (mode)
            predictions.append(statistics.mode(votes))
        # Return the final predictions as a numpy array
        return np.array(predictions)

print("Please copy this code and replace the content of your 'random_forest_classifier_scratch.py' file with it.")